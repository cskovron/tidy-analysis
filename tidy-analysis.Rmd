---
title:  |
        | Tools for tidy analysis 
author: |
        | Christopher Skovron
        | Northwestern University 
        | `R` User Group
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  revealjs::revealjs_presentation: 
    theme: simple
    highlight: haddock
    center: false
    transition: none
    fig_width: 5
    fig_height: 4
---

```{r setup, include=FALSE}
library(knitr)
```


## Tidying up the analysis stage

See these slides and the Rmd on github:
http://github.com/cskovron/tidy-analysis


## Tidy data principles 

![](./figures/tidy-data.png)

## The modeling stage 

![](./figures/data-science-model.png)


## The problem: a lack of convention in analysis and modeling packages 

- Lack of conventions for how models are specified in calls to modeling functions
- Lack of conventions for what output looks like and how we work with it 


## Fortunately, an emerging set of conventions on tidy principles

- this will be housed under [tidymodels](https://www.tidyverse.org/articles/2018/08/tidymodels-0-0-1/)
- [A draft set of tidy conventions for modeling packages](https://tidymodels.github.io/model-implementation-principles/index.html)
- But this is still largely under development


## Tools 

- `broom` - models
- `infer` - hypothesis testing
- `corrr` - tidy analysis of correlations 

## Additional modeling packages emerging 

- `recipes` simplifies getting design matrices
- `tidybayes` and `tidyposterior` 
- e.g., [`parsnip`](https://topepo.github.io/parsnip/), a tool to unify model specification across packages



## Today, some examples with `corrr`, `infer`, and `broom`

## Check for some installations

```{r, eval=FALSE}
install.packages(c("tidyverse", "corrr","infer","dotwhisker"))
```

## Load packages we will need 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(corrr)
library(infer)
library(dotwhisker)
```




## Tidy correlations with `corrr`

```{r, message=FALSE, warning=FALSE}
#install.package('corrr')
library(corrr)
d <- correlate(mtcars)
d
```


## `corrr` makes exploring correlations easy + tidy 

```{r}
# Filter rows to occasions in which cyl has a correlation of .7 or more with
# another variable.
d %>% filter(cyl > .7)

```


## `corrr` makes exploring correlations easy + tidy 
```{r}
# Select the mpg, cyl and disp columns (and rowname)
d %>% select(rowname, mpg, cyl, disp)
```
 

## Lots of operations you can do on a `cor_df`

- Internal changes (cor_df out)
    - shave() the upper or lower triangle (set to NA).
    - rearrange() the columns and rows based on correlation strengths.

- Reshape structure (tbl or cor_df out):
    - focus() on select columns and rows.
    - stretch() into a long format.

- Output/visualisations (console/plot out):
    - fashion() the correlations for pretty printing.
    - rplot() a shape for each correlation.
    - network_plot() a point for each variable, joined by paths for correlations.


## Tidy correlation output 

```{r}
d %>%
  focus(mpg:drat, mirror = TRUE) %>%  # Focus only on mpg:drat
  shave() %>% # Remove the upper triangle
  fashion()   # Print in nice format 
```

## Visualize correlations 
```{r}
d %>%
  focus(mpg:drat, mirror = TRUE) %>%
  rearrange(absolute = FALSE) %>% 
  shave() %>%
  rplot()
```



## Using `infer` for tidy hypothesis testing

[`infer`](https://infer.netlify.com/reference/index.html) provides tidy tools for common hypothesis testing tasks including t-tests, chi-squared tests, etc. It's really powerful for randomization/permutation inference and bootstrapping. For parametric tests, often ok to use base stats functions and then use `broom`, which we'll review in the next section, to tidy them. 

![](./figures/infer_hex.png){width=250px}







## Set up mtcars for example 

Set up mtcars for the example analysis - mutate vars to factors

```{r}
mtcars <- as.data.frame(mtcars) %>%
  mutate(cyl = factor(cyl),
          vs = factor(vs),
          am = factor(am),
          gear = factor(gear),
          carb = factor(carb))
```

## `infer` functions

- `specify()` - specify response and explanatory variables, using formula
- `hypothesize()` - declare a null hypothesis (typically "independence")
- `generate()` - generate permuatations/bootstraps 
- `calculate()` - calculate summary statistics 

## Things we can calculate() include

"mean", "median", "sd", "prop", "diff in means",
  "diff in medians", "diff in props", "Chisq", "F", "slope", "correlation",
  "t", "z"

## Difference in means: the old way
 
```{r}
t.out <-  t.test(mpg ~ am, data = mtcars)
t.out
```

## Accessing stuff here is a chore
```{r}
t.out$statistic
```
```{r}
t.out$p.value
```
```{r}
t.out$estimate
```


## Difference in means with `infer`
Use the `t_test()` function: 
```{r}
mtcars %>%
  t_test(mpg ~ am, order = c("1", "0"), alternative = "less")
```

## Or bootstrap it quickly and easily 
Note the non-formula interface: 
```{r}
mtcars %>%
  specify(response = mpg, explanatory = am) %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "diff in means", order = c("1", "0"))
```






## `broom` - tidy up modeling output 

![](./figures/broom.png){width=250px}


## `broom` tidies up regression output 

- output from `lm`, `glm`, `t.test` is ugly and not standardized
- `broom` transforms into easy-to-use matrices
- makes manipulating model output much easier
- especially useful if you're simulating a lot of models 




## Tidying functions

- `tidy`: constructs a data frame that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for `multtest` functions.
- `augment`: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.
- `glance`: construct a concise *one-row* summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.


## Example 

```{r lmfit}
lmfit <- lm(mpg ~ wt, mtcars)
lmfit
summary(lmfit)
```

## Tidy the output 

Instead, you can use the `tidy` function, from the broom package, on the fit:

```{r}
library(broom)
tidy(lmfit)
```

## Tidy the output 

This gives you a data.frame representation. Note that the row names have been moved into a column called `term`, and the column names are simple and consistent (and can be accessed using `$`).

## Augment the output 

Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use `augment`, which augments the original data with information from the model:

```{r}
augment(lmfit)
```

Note that each of the new columns begins with a `.` (to avoid overwriting any of the original columns).

## Glance at the output 


Finally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the `glance` function:

```{r}
glance(lmfit)
```



## Generalized linear and non-linear models

These functions apply equally well to the output from `glm`:

```{r glmfit}
glmfit <- glm(am ~ wt, mtcars, family="binomial")
tidy(glmfit)
augment(glmfit)
glance(glmfit)
```

## Generalized linear and non-linear models

These functions also work on other fits, such as nonlinear models (`nls`):

```{r}
nlsfit <- nls(mpg ~ k / wt + b, mtcars, start=list(k=1, b=0))
tidy(nlsfit)
augment(nlsfit, mtcars)
glance(nlsfit)
```

## Hypothesis testing

The `tidy` function can also be applied to `htest` objects, such as those output by popular built-in functions like `t.test`, `cor.test`, and `wilcox.test`.

```{r ttest}
tt <- t.test(wt ~ am, mtcars)
tidy(tt)
```

## Hypothesis testing

Some cases might have fewer columns (for example, no confidence interval):

```{r}
wt <- wilcox.test(wt ~ am, mtcars)
tidy(wt)
```

## Hypothesis testing

Since the `tidy` output is already only one row, `glance` returns the same output:

```{r}
glance(tt)
glance(wt)
```


## Tidy bootstrapping with `broom`


## Example: weight and mileage in `mtcars`

```{r, echo=TRUE}
library(ggplot2)
data(mtcars)
ggplot(mtcars, aes(mpg, wt)) + geom_point()
```


## Fit an nls model

```{r, echo=TRUE}
nlsfit <- nls(mpg ~ k / wt + b, mtcars, start=list(k=1, b=0))
summary(nlsfit)
```

## model 
```{r}
ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_line(aes(y=predict(nlsfit)))
```


## We want to bootstrap the confidence intervals


```{r, echo=TRUE, message = FALSE}
library(dplyr)
library(broom)
set.seed(2014)
bootnls <- mtcars %>% bootstrap(100) %>%
    do(tidy(nls(mpg ~ k / wt + b, ., start=list(k=1, b=0))))
```

## We get a clean `data.frame` that summarizes each replication 

```{r, echo=TRUE}
bootnls
```

## use the percentile method to calculate uncertainty 

```{r, echo=TRUE}
alpha = .05
bootnls %>% group_by(term) %>% 
  summarize(low=quantile(estimate, alpha / 2),
  high=quantile(estimate, 1 - alpha / 2))
```

## Or you can use `augment` to visualize the uncertainty in the curve:

```{r, echo=TRUE}
bootnls_aug <- mtcars %>% bootstrap(100) %>%
    do(augment(nls(mpg ~ k / wt + b, ., start=list(k=1, b=0)), .))

ggplot(bootnls_aug, aes(wt, mpg)) + geom_point() +
    geom_line(aes(y=.fitted, group=replicate), alpha=.2)
```


## `broom` and "the secret weapon"

- [Gelman's "secret weapon"](https://andrewgelman.com/2005/03/07/the_secret_weap/)
- Fit the same model repeatedly to different datasets 
- Visualize a single predictor across each group 

## Use `dotwhisker` and `broom` to use the secret weapon in a tidy way 

## 

```{r, tidy = TRUE}
# Estimate models across many samples, put results in a tidy data frame
by_clarity <- diamonds %>% 
  group_by(clarity) %>%
  do(broom::tidy(lm(price ~ carat + cut + color, data = .))) %>%
  ungroup %>% 
  rename(model = clarity)

# Generate a 'secret weapon' plot of the results of diamond size
secret_weapon(by_clarity, "carat")
```



